% \VignetteEngine{knitr::knitr}
% \VignetteEncoding{UTF-8}
% \VignetteIndexEntry{dtComb}

\documentclass[10pt]{article}
\usepackage[left=3cm, top=2.5cm, right=2.5cm, bottom=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=black,
  filecolor=magenta,      
  urlcolor=blue
}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage[numbers]{natbib}
\usepackage{nameref} % cross-reference for section names.
\usepackage{booktabs}
\usepackage{caption}

\usepackage{authblk}  % authors and affiliations
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}

%%% New Commands %%%
\newcommand{\dtComb}{\textit{dtComb}}
% \newcommand{\Biocpkg}[1]{\href{https://www.bioconductor.org/packages/release/bioc/html/#1.html}{\texttt{#1}}}
\newcommand{\CRANpkg}[1]{\href{https://cran.r-project.org/web/packages/#1/index.html}{\texttt{#1}}}
\newcommand{\Rfunction}[1]{\texttt{#1}}
\newcommand{\Rcode}[1]{\texttt{#1}}
\newcommand{\Rclass}[1]{\texttt{#1}}
\newcommand{\software}[1]{\texttt{#1}}

%%%% Set knitr options %%%%
<<echo=FALSE>>=
library(knitr)
opts_chunk$set(tidy = FALSE, dev = "pdf",  message = FALSE, fig.align = "center", cache = FALSE)
@

%%%% Load required packages %%%%


\title{\textbf{dtComb: A Comprehensive R Package for Combining Two Diagnostic Tests}}

\author[1,2]{Serra İlayda YERLİTAŞ }
\author[2]{Serra Berşan GENGEÇ }
\author[1,2]{Gözde ERTÜRK ZARARSIZ}
\author[3]{Selçuk KORKMAZ}
\author[1,2${}^{\dagger}$]{Gökmen ZARARSIZ}


\affil[1]{Erciyes University Faculty of Medicine Department of Biostatistics, Kayseri, TURKEY \vspace*{0.3em}}
\affil[2]{Erciyes University Drug Application and Research Center (ERFARMA) , Kayseri, TURKEY \vspace*{0.3em}}
\affil[3]{Trakya University Faculty of Medicine Department of Biostatistics, Edirne, TURKEY \vspace*{0.3em}}

\renewcommand\Authands{ and }

\date{
  \today
}

\begin{document}

\maketitle
\vspace*{10pt}

\begin{abstract}
\CRANpkg{dtComb} is a comprehensive R package developed to combine two different diagnostic tests and offers users an extensive range of distinct methods brought together. Using the \CRANpkg{dtComb} package, researchers are able to apply standardization to their data and combine these diagnostic tests with 143 combination methods available in the package by loading the diagnostic tests and the dataset containing the reference test. Combination methods included in the package are gathered under 4 main groups:  Linear Combination Methods (\Rfunction{linComb}), Nonlinear Combination Methods (\Rfunction{nonlinComb}), Mathematical Operators (\Rfunction{mathComb}) and Machine Learning Algorithms (\Rfunction{mlComb}). Within the scope of Linear Combination approaches there are 8 combination methods gathered and applied from literature. Nonlinear Combination Methods include several statistical approaches such as polynomial regression, penalized regression methods and splines. In addition to these methods, the interactions between the diagnostic tests can also be incorporated to the combination model. Arithmetic operators are included in the group of Mathematical Operators as well as 8  \texttt{distance measures} that can be used according to the data structure. The final group is Machine Learning Algorithms, which consist of 113 machine learning algorithms from the \CRANpkg{caret} package that are suitable for the \CRANpkg{dtComb} data structure. 

For data standardization, five different standardization methods have been prepared for linear, non-linear combination methods and mathematical operators  in the package. These methods allows users to standardize the data according to \texttt{range}, \texttt{z score}, \texttt{t score}, \texttt{mean}, and \texttt{standard deviation} calculated from the loaded dataset. For the machine learning approaches, the \texttt{preprocessing} methods included in the \CRANpkg{caret} package can be used within the \CRANpkg{dtComb} package for standardization.

In terms of \texttt{resampling} methods, Cross-Validation, Bootstrapping, and Repeated Cross-Validation  are available to be used with linear and nonlinear combination methods for model building and  performance evaluation. For ML algorithms, the \texttt{resampling} methods in the \CRANpkg{caret} package are applicable. 

Finally, using the models built with these aforementioned functions, the \texttt{predComb} function, predicts class labels and combination scores of a given set of  test samples. 

The \CRANpkg{dtComb} package is user-friendly, effortless, and is currently the most comprehensive single package developed in the literature to combine diagnostic tests. This vignette has been created to guide researchers on how to use this package.

\vspace{1em}

\noindent\textbf{dtComb version:} \Sexpr{packageDescription("dtComb")$Version}

\end{abstract}

\section{Introduction}
Diagnostic tests are widely used for purposes of distinguishing diseases from each other and making the correct diagnosis for the patients. Therefore, the role of the diagnostic tests in  making decisions is crucial. In addition to having an essential role in medical diagnosis,  these tests also contribute to planning the proper treatment while reducing treatment costs.  The diagnostic accuracy performance and reliability of these diagnostic tests are taken into account  when making these tests widely available. For some conditions, more than one diagnostic test may be available. Some of these diagnostic tests may even replace existing methods because they have better performance. In many studies, new diagnostic test performance measures with superior performance were obtained by using multiple diagnostic test instead of one. Various approaches to combining diagnostic tests are available  in the literature. The \dtComb{} package includes diverse  combination methods, data standardization, and resampling methods. 
This vignette will demonstrate how to combine two different diagnostic tests with different combination methods. \dtComb{}package is loaded as below:

<<>>=
library(dtComb)
@

\section{Preparing the input data}
The methods available in this package are structured to require a \Rclass{DataFrame} with two categories (negative/positive, present/absent) as the results of a reference test used in the precision of the disease and a continuous quantitative structure for the diagnostic tests. In this demonstration, we will be using the dataset named exampleData1 that is included in this package. The exampleData1 dataset contains data from patients admitted to the Erciyes University Medical Faculty’s General Surgery Department with complaints of abdominal pain. Dataset includes the leukocyte counts and D-dimer levels of 225 patients (115 females, 110 males) of two groups that indicates the results of the reference test. The first group had 115 ($51,1\%$) patients who needed immediate laparotomy, and the second group had 110 ($49,9\%$) patients who did not need immediate laparotomy. Conventional treatment is assessed according to this grouping, and the patients operated on based on their postoperative pathologies are assigned to the first group. In contrast, the patients with a negative laparotomy are assigned to the second group \citep{akyildiz2010value}.\newline
The dataset exampleData1 is called using the  \Rfunction{data} function to set up the model. The data set is structured as follows.

<<eval = TRUE, echo=TRUE>>=
data(exampleData1)
head(exampleData1)
@

In this demonstration a training set and a test set will be used to show the capabilites of this package. The training set will consist of $75\%$ of the data set and will be used to train classification models as well as to compare different model performances. The remaining of the data set will be saved as a test set to later demonstrate the prediction functionality using the trained model parameters of selected classification models. The \texttt{train} set is constructed as below, and the \texttt{test} set will be set up later on.

<<>>=
# # train set from the exampleData1
set.seed(2128)
inTrain <- caret::createDataPartition(exampleData1$group, p = 3/4, list = FALSE)
trainData <- exampleData1[inTrain, ]
head(trainData)
@
When the contents of the \texttt{train} set is examined it can be seen that out of 170 observations in this set, 83 patients require laparotomy, while 87 patients do not require laparotomy according to the group column.\newline
Functions used for training are designed to take diagnostic tests and reference test results as two different inputs. In this step, the training set obtained from the sample data set is divided into diagnostic test results (\texttt{markers}) and reference test results (\texttt{status}). During this division process, the referencce test results (\texttt{status}) are transformed into factor variables.

<<>>=
markers <- trainData[, -1]
status <- factor(trainData$group, levels = c("not_needed", "needed"))
@

\section{Avaible Methods}
There are various methods for combining diagnostic tests. One of the extensively used method is comparing the two diagnostic tests. However, models with notably higher accuracy can be obtained using statistical methods and machine learning algorithms. The \dtComb{} package includes 143 methods for combining diagnostic tests. These methods are divided into linear methods, non-linear methods, mathematical operators, and machine learning algorithms. \newline

\subsection{Linear Combination Methods:}
\begin{itemize}
  \item \textbf{Scoring based on Logistic Regression (\texttt{scoring})}: Combination score is obtained using the slope values of the relevant logistic regression model, slope values are rounded to the number of digits taken from the user. \citep{leon2006bedside}.
    \begin{gather*} 
    l = \log_b\left(\frac{p}{1 - p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2  \\
    Combination\;Score = \beta_1 x_1 + \beta_2 x_2
    \end{gather*}
    
 \item \textbf{Su \& Liu's method  (\texttt{SL})}: Su and Liu combination score is obtained by using Fisher's discriminant function under the assumption of multivariate normal distribution model and proportionate covariance matrices \citep{su1993linear}.
    \begin{gather*} 
    Contol\; group = X \sim N(\mu_x , \sum) \\
    Disease \: group = Y \sim N(\mu_y ,\sigma ^ 2 \sum) \\
    Fisher's\; discriminant \;coefficient = (\alpha , \beta) \propto (\mu_y- \mu_x) ^ T{\sum_{}^{\;\;\;\;\;\;\;\;-1}} \\
    Combination \;Score = \alpha x_1 + \beta x_2
    \end{gather*}
  \item \textbf{Logistic Regression (\texttt{logistic})}: Combination score obtained by fitting a logistic regression model \citep{walker1967estimation}.
    \begin{gather*} 
    l = \log_b\left(\frac{p}{1 - p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2
    \end{gather*}

  \item \textbf{Min-Max method (\texttt{minmax})}: Linearly combines the minimum and maximum values of the markers by finding a parameter $\lambda$ that maximizes corresponding Mann-Whitney statistic. \citep{liu2011min}.
    \begin{gather*}
    Control \; group: X_i = (X_1i, X_2i), i = 1, 2, ..., n \\
    Disease \; group : Y_j = (Y_1j, Y_2j), j = 1, 2, ..., m \\ 
    maximize\;W( \lambda ) = \left(\frac{1}{mn}\right) {\sum_{i=1}^{n} {\sum_{j=1}^{m}}I(Y_{j,max} +  \lambda Y_{j,min} > X_{i,max} +          \lambda X_{i,min})} \\
    Combination \; Score = x_{max} +  \lambda x_{min}
    \end{gather*}
    
  \item \textbf{Pepe \& Thompson's method (\texttt{PT})}:Pepe and Thompson combination score obtained by proportioning the slope values the relevant logistic regression model to obtain the parameter $\lambda$ \citep{pepe2000combining}.
    \begin{gather*} 
    l = \log_b\left(\frac{p}{1 - p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \\ 
    \lambda  = \beta_2/\beta_1 \\
    Combination \; Score = x_1 + \lambda x_2
    \end{gather*}
    
\item \textbf{Pepe, Cai \& Langton's method (\texttt{PCL})}: Pepe, Cai and Langton combination score obtained by using AUC as the parameter of a logistic regression model \citep{pepe2006combining}.
    \begin{gather*} 
    Control \; group: X_i = (X_1i, X_2i), i = 1, 2, ..., n  \\
    Disease \; group = Y_j = (Y_1j, Y_2j), j = 1, 2, ..., m \\ 
    maximize\; W(\lambda) = \left(\frac{1}{mn}\right) {\sum_{i=1}^{n} {\sum_{j=1}^{m}}I(Y_{1j} + \lambda Y_{2j} > X_{1i} + \lambda X_{2i})     + \left(\frac{1}{2} \right) I(Y_{1j} + \lambda Y_{2j} = X_{1i} + \lambda X_{2i})} \\
    Combination \; Score = x_1 + \lambda x_2
    \end{gather*}
    
 \item \textbf{Minimax approach (\texttt{minimax}):} Combination score obtained with the Minimax procedure; \emph{t} parameter is chosen as the value that gives the maximum AUC from the combination score. \citep{sameera2016binary}.
    \begin{gather*} 
    Contol \;group: X_i =  (X_{1i}, X_{2i}), i = 1, 2, ... , n \\
    Disease \; group = Y_j =  (Y_{1j}, Y_{2j}), j = 1, 2, ... , m \\
    (b_1, b_2) = {[t {\sum_{\;\;\;\;\;\;\;D}} + (1 - t) \sum_{\;\;\;\;\;\;\;C}] ^ {-1}}{(\mu_D - \mu_C)} \\
    Combination\; Score = b_1 x_1 + b_2 x_2
    \end{gather*}
    
  \item \textbf{Todor \& Saplacan’s method (\texttt{TS})}: Combination score obtained by using the trigonometric functions of the $\Theta$ value that optimizes the corresponding AUC \citep{todor2014tools}.
    \begin{align*} 
    Combination \: Score = sin(\theta)x_1 + cos(\theta)x_2
    \end{align*}
\end{itemize} 

\subsection{Nonlinear Combination Methods:}
\begin{itemize}
  \item \textbf{Logistic Regression with Polynomial Feature Space (\texttt{polyreg})}: The method builds a logistic regression model with the feature space created and returns the probability of a positive event for each observation.
  
  \item \textbf{Ridge Regression with Polynomial Feature Space (\texttt{ridgereg})}: Ridge regression is a shrinkage method used to estimate the coefficients of highly correlated variables and in this case the polynomial feature space created from two biomarkers. For the implementation of the method, \CRANpkg{glmnet} library is used with two functions: \Rfunction{cv.glmnet} to run a cross validation  model to determine the tuning parameter $\lambda$ and \Rfunction{glmnet} to fit the model with the selected tuning parameter \citep{friedman2010regularization}. 

  \item \textbf{Lasso Regression with Polynomial Feature Space (\texttt{lassoreg})}: Lasso regression is also a shrinkage method with one difference is that at the end this method returns the coefficients of some features as 0, makes this method useful for feature elimination as well. The implementation is similar to Ridge regression, cross validation for parameter selection and model fit are implemented with \CRANpkg{glmnet} library \citep{friedman2010regularization}.

  \item \textbf{Elastic Net Regression with Polynomial Feature Space (\texttt{elasticreg})}: Elastic Net regression is obtained by combining the penalties of Ridge and Lasso regression to get the best of both models. The model again includes a tuning parameter $\lambda$ as well as a mixing parameter $\alpha$ taken form the user which takes a value between 0 (ridge) and 1 (lasso) to determine the weights of the loss functions of Ridge and Lasso regressions \citep{friedman2010regularization}
 

  \item \textbf{Splines (\texttt{splines})}: With the applications of regression models in a polynomial feature space the second non-linear approach to combining biomarkers comes from applying several regression models to the dataset using a function derived from piecewise polynomials. Splines are implemented with degrees of freedom and degrees of the fitted polynomials taken from the user. For the implementation \CRANpkg{splines} library is used to build piecewise logistic regression models with base splines \citep{team2013r}. 

  \item \textbf{Generalized Additive Models with Smoothing Splines and Generalized Additive Models with Natural Cubic Splines (\texttt{sgam} ve \texttt{nsgam})}: In addition to the basic spline structure, Generalized Additive Models are applied with natural cubic splines and smoothing splines using the \CRANpkg{gam} library [10] in R \citep{hastie2019gam}.
\end{itemize}

One of the non-linear approaches is to incorporate any interaction that may exist between the two diagnostic tests into the model built. Incorporating the interaction of variables to a regression model has benefits when there is a correlation between the diagnostic tests. To include interaction while building the model, the \texttt{include.interact} option will be selectes as \texttt{TRUE} in the nonlinComb function.

\subsection{Mathematical Operators:}


\begin{itemize}
  \item \textbf{Arithmetic Operators} : \texttt{add}, \texttt{subtract}, \texttt{multiply} ve \texttt{divide} methods represent basic arithmetic operators.    
  \item \textbf{Distance Measures}: The distance measures included in the package are \texttt{Euclidean}, \texttt{Manhattan}, \texttt{Chebyshev}, \texttt{Kulczynski d}, \texttt{Lorentzian}, \texttt{Avg}, \texttt{Taneja}, and  \texttt{Kumar-Johnson}. The mathematical calculation methods for these measures are as follows. \citep{drost2018philentropy}.
    \begin{itemize}
      \item \textbf{Euclidean}(\texttt{euclidean}) $= {\sqrt{\sum_{i=1}^{N}|P_i-Q_i|^2}}$
      \item \textbf{Manhattan}(\texttt{manhattan}) $ = \sum_{i=1}^{N}|P_i-Q_i|$
      \item \textbf{Chebyshev}(\texttt{chebyshev}) $ = max|P_i-Q_i|$
      \item \textbf{Kulczynski d}(\texttt{kulczynski-d}) $ = \frac{\sum_{i=1}^{N}|P_i-Q_i|}{\sum_{i=1}^{N}min(P_i,Q_i)}$ 
      \item \textbf{Lorentzian}(\texttt{lorentzian}) $ = \sum_{i=1}^{N}\ln(1+|P_i-Q_i|)$
      \item \textbf{Avg}(\texttt{avg}) $ =  \frac{\sum_{i=1}^{N}|P_i-Q_i| + max|P_i-Q_i|}{2}$
      \item \textbf{Taneja}(\texttt{taneja}) $ = \sum_{i=1}^{N}\frac{P_i+Q_i}{2} .\log(\frac{P_i+Q_i}{2\sqrt{P_i.Q_i}})$
      \item \textbf{Kumar-Johnson}(\texttt{kumar-johnson}) $ = \sum_{i=1}^{N}{\frac{(P_i^2-Q_i^2)^2}{2.(P_i.Q_i)^{3/2}}}$
    \end{itemize}
  \item \textbf{Exponential functions}: These methods, in which one of the two diagnostic tests is taken as base and the other as an exponent, are indicated by the names \texttt{baseinexp}($markers_1^{markers_2}$) and \texttt{expinbase} ($markers_2^{markers_1}$).
\end{itemize}

Another option given is to apply a transformation on the markers before using the methods available in mathematical operators. The types of transformations that can applied are \emph{logarithmic} (\texttt{log}), \emph{exponential} (\texttt{exp}), \emph{sine} (\texttt{sin}), and \emph{cosine} (\texttt{cos}).  The \texttt{power.transform} option can also be used to optimize the \textbf{AUC} on biomarkers. When the \texttt{power.transform} option is selected as \texttt{TRUE}, the power of the biomarkers are taken in increments of 0.1 step size within the [-3, 3] range and the method chosen by the user is applied to the diagnostic tests, the point with maximum \textbf{AUC} is returned and chosen for the final model.

\subsection{Machine Learning Algorithms:}
113 machine learning algorithms available in the \CRANpkg{caret} library, which are used to train classification models using machine learning algorithms and make predictions using these models, are used within the scope of \Rfunction{mlComb} function \citep{kuhn2015caret}.

The \Rfunction{availableMethods} function can be used to see the machine learning algorithms included in the package.

\section{Standardization}
Standardization is one of the important steps in data analysis when diagnostic tests with different units are used. It is better to have the data in the same unit or range in most cases. Standardization is provided as an option that can be applied with all models, although some combination methods available in the \dtComb{} package require standardization by default. Standardization methods have been prepared for \Rfunction{linComb}, \Rfunction{nonlinComb}, and \Rfunction{mathComb} functions without depending on other libraries, and the details of the methods used for standardization are given below:
\begin{itemize}
  \item \textbf{range}: Standardization to a range between 0 and 1
  \item \textbf{zScore}: Standardization using z scores with mean equals to 0 and standard deviation equals to 1
  \item \textbf{tScore}: Standardization using T scores. The range varies between usually 20 and 80
  \item \textbf{mean}: Standardization with sample mean equals to 1
  \item \textbf{deviance}: Standardization with sample standard deviation equals to 1
\end{itemize}

The standardization process for the \Rfunction{mlComb} function is performed using the \Rfunction{preprocessing} function in the \CRANpkg{caret} package.


\section{Resampling}
The resampling options for parameter selection in linear and non-linear combinations are specified with the \texttt{resample} argument. Cross-Validation (\texttt{cv}), Repeated Cross-Validation (\texttt{cv}), and Bootstrapping (\texttt{boot}) methods are given as resampling options in these combination models. For the Cross-validation option, \texttt{nfolds} is given to determine the number of groups to be created; for the Repeated cross validation option \texttt{nrepeat} is used to specify the number of repetitions, and for the Bootstrapping option \texttt{niters} is given to determine the number of subgroups to be created.

Resampling for the \Rfunction{mlComb} function is performed using the \Rfunction{resampling} function included in the \CRANpkg{caret} package. Detailed information is available in the relevant section of the \CRANpkg{caret} package \citep{kuhn2015caret}.

\section{Cutoff and Direction}
AUC values and ROC curves are considered when evaluating the model performance in the training process. The \texttt{direction} argument determines the direction of the ROC curve and is given as an input to the relevant function. Its default value is set to \texttt{auto}. The cutoff point in the ROC curve drawn to determine the AUC value is controlled by the \texttt{cutoff.method} argument. \texttt{youden} and \texttt{roc01} method options are available for this argument \citep{robin2011proc}.

\section{Model building}
dtComb has 4 different functions (\texttt{linComb}, \texttt{nonlinComb}, \texttt{mlComb}, \texttt{mathComb}) to build and evaluate models. These functions establish the appropriate model in line with the methods chosen by the user and present the performance to the user. For the building of the model, previously prepared markers and status variables, the state where the \texttt{event} to be classifed occurred, the method to be used, and the arguments subject to the method are given as input to the functions. One function from each group is prepared as examples that will be built using the specifications below:

\begin{itemize}
  \item For linear combination function, \texttt{scoring} (\texttt{ndgits = 2}) method will be used together with \texttt{resample = cv} (\texttt{nfold = 5}) and \texttt{range} as the standardization method
  \item For non-linear combination function, \texttt{lassoreg} (\texttt{include.interact = TRUE}) method will be used together with \texttt{boot} (\texttt{niters = 10}) as the resampling method
  \item For machine learning combination function, \texttt{knn} method will be used together with \texttt{repeatedcv} (\texttt{nfolds = 10, nrepeats = 5}) as the resampling method
  \item For the mathematical operators function, \texttt{distance} (\texttt{distance = “euclidean”}) method will be used.
\end{itemize}


<<fig.height=4.8, fig.width=5>>=
set.seed(2128)

#linComb Function
fit.lin <- linComb(markers = markers, 
                      status = status, 
                      event = "needed", 
                      method = "scoring", 
                      resample = "cv",
                      standardize = "range", 
                      ndigits = 2, direction = "auto", 
                      cutoff.method = "youden")
@

<<fig.height=4.8, fig.width=5>>=
#nonlinComb Function
fit.nonlin <- nonlinComb(markers = markers, 
                            status = status, 
                            event = "needed", 
                            method = "lassoreg", 
                            include.interact = "TRUE",
                            resample = "boot", 
                            direction = "auto", 
                            cutoff.method = "youden")
@

<<fig.height=4.8, fig.width=5>>=
#mlComb Function
fit.ml <- mlComb(markers = markers, 
                     status = status, 
                     event = "needed",
                     method = "knn",
                     resample = "repeatedcv", nfolds = 10, nrepeats = 5,
                     preProcess = c("center", "scale"), 
                     direction = "<", cutoff.method = "youden")
@

<<fig.height=4.8, fig.width=5>>=
#mathComb Function
fit.math <- mathComb(markers = markers,
                         status = status,
                         event = "needed",
                         method = "distance",
                         distance = "euclidean",
                         direction = "<",
                         cutoff.method = "youden")
@


\section{Comparing the performance of classifiers}
As seen in Table \ref{tbl:Res}, the results of the four models set up, the combined diagnostic tests have performed higher than the single diagnostic tests. Among the models that have been built, the K-Nearest Neighbors \texttt{knn} method, one of the machine learning algorithms, returns the highest AUC value. The (\texttt{knn}) method parameters that gives the best results will be used in the next step to make predictions on the test data.

\begin{table}[ht]
\centering
\caption{Combination results for train data}
\label{tbl:Res}
\begin{tabular}{p{4cm}p{2cm}c}
\toprule
Metot & AUC & Accuracy \\
\midrule
ddimer & 0.822 & 0.771 \\
log(leukocyte) & 0.795 & 0.765 \\
scoring & 0.878 & 0.824 \\
lassoreg & 0.908 & 0.835 \\
knn & 0.939 & 0.745 \\
distance(euclidean) & 0.880 & 0.806 \\
\bottomrule
\end{tabular}
\end{table}




\section{Predicting the class labels of test samples}
The class labels of the test cases are estimated based on the parameters of the model trained, e.g. If we are training a model using the \texttt{knn} method, one of the machine learning algorithms, the labels of the test set are estimated by the parameters we obtained from the training set. However, the important point is that the test set must have gone through the same standardization stages as the training set. The \Rfunction{predComb} function makes predictions in line with the training model by applying this standardization process in the test set. The test data set is created from the exampleData1 sample data as follows, and the \Rfunction{predComb} function is applied to this data set.

<<concordance=TRUE>>=
testData <- exampleData1[-inTrain, -1]
@

<<>>=
predComb(fit.nonlin, testData)
@

The \Rfunction{predComb} function returns the combined score of the applied method and the estimated label for the models trained using the \Rfunction{linComb}, \Rfunction{nonlinComb}, or \Rfunction{mathComb} functions. For the models trained using \Rfunction{mlComb} function, the probability of positive and negative cases for each test observation is given as output.

\section{Session info}
<<>>=
sessionInfo()
@

\bibliographystyle{unsrtnat}
\bibliography{dtComb}
\end{document}
